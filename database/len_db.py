import json
import uuid

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

# ==== âœ… CONFIG ====
NDJSON_PATH = "NLP/chunking/length/len.json"
FAISS_INDEX_PATH = "NLP/save_local_db/len/vector_index.faiss"
METADATA_PATH = "NLP/save_local_db/len/vector_metadata.json"
MODEL_NAME = "intfloat/multilingual-e5-large"
# ===================

# âœ… Load SentenceTransformer model
print(f"ðŸ“¦ Loading SentenceTransformer model: {MODEL_NAME} ...")
sentence_model = SentenceTransformer(MODEL_NAME)

# âœ… FAISS index (embedding_dim, cosine similarity â†’ normalize trÆ°á»›c)
dimension = sentence_model.get_sentence_embedding_dimension()
index = faiss.IndexFlatIP(dimension)  # cosine similarity via inner product

# âœ… Metadata list (song song vá»›i vectors)
metadata_store = []


# âœ… HÃ m táº¡o embedding tá»« SentenceTransformer
def get_embedding(text: str):
    vec = sentence_model.encode(text, normalize_embeddings=True)
    return np.array(vec, dtype=np.float32)


# âœ… Load vÃ  lÆ°u vector + metadata
def upload_chunks_to_faiss(json_path: str):
    with open(json_path, "r", encoding="utf-8") as f:
        all_docs = json.load(f)  # âœ… load full JSON list

    for doc in tqdm(all_docs, desc="ðŸš€ Embedding & Saving"):
        url = doc.get("url", "")
        chunks = doc.get("chunks", [])

        for i, chunk in enumerate(chunks):
            text = chunk.get("content", "").strip()
            if not text:
                continue

            vec = get_embedding(text)
            index.add(np.expand_dims(vec, axis=0))

            metadata_store.append(
                {
                    "id": str(uuid.uuid4()),
                    "url": url,
                    "text": text,
                    "chunk_index": i,
                }
            )


# âœ… Run & Save
if __name__ == "__main__":
    upload_chunks_to_faiss(NDJSON_PATH)

    # LÆ°u FAISS index
    faiss.write_index(index, FAISS_INDEX_PATH)
    print(f"âœ… FAISS index saved to {FAISS_INDEX_PATH}")

    # LÆ°u metadata song song
    with open(METADATA_PATH, "w", encoding="utf-8") as f:
        json.dump(metadata_store, f, indent=2, ensure_ascii=False)
    print(f"âœ… Metadata saved to {METADATA_PATH}")
