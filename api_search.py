import json
import os
from operator import itemgetter

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# ==== âœ… CONFIG ====
MODEL_NAME = "intfloat/multilingual-e5-large"
TOP_K = 5

# âœ… Láº¥y Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i Ä‘áº¿n thÆ° má»¥c hiá»‡n táº¡i
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))

# âœ… Chuyá»ƒn sang folder `save_local_db/sem_len` Ä‘Ãºng tuyá»‡t Ä‘á»‘i
BASE_DB_PATH = os.path.join(CURRENT_DIR, "save_local_db", "sem_len")
FAISS_INDEX_PATH = os.path.join(BASE_DB_PATH, "vector_index.faiss")
METADATA_PATH = os.path.join(BASE_DB_PATH, "vector_metadata.json")

# âœ… Kiá»ƒm tra tá»“n táº¡i
if not os.path.exists(FAISS_INDEX_PATH):
    raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y FAISS index táº¡i: {FAISS_INDEX_PATH}")
if not os.path.exists(METADATA_PATH):
    raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y metadata táº¡i: {METADATA_PATH}")
# ===================

# âœ… Load model, index, metadata
print("ğŸ“¦ Loading model/index/metadata...")
model = SentenceTransformer(MODEL_NAME)
index = faiss.read_index(FAISS_INDEX_PATH)

with open(METADATA_PATH, "r", encoding="utf-8") as f:
    metadata = json.load(f)

corpus_texts = [doc["text"] for doc in metadata]

# âœ… TF-IDF vectorizer
print("ğŸ§  Building TF-IDF vectorizer...")
tfidf_vectorizer = TfidfVectorizer().fit(corpus_texts)
corpus_sparse = tfidf_vectorizer.transform(corpus_texts)


# âœ… Embedding query
def embed_dense(text: str) -> np.ndarray:
    vec = model.encode(text, normalize_embeddings=True)
    return np.array(vec, dtype=np.float32).reshape(1, -1)


def embed_sparse(text: str):
    return tfidf_vectorizer.transform([text])


# âœ… Dense search
def dense_search(query: str, top_k: int):
    query_vec = embed_dense(query)
    scores, indices = index.search(query_vec, top_k)
    return [(int(i), float(s)) for i, s in zip(indices[0], scores[0]) if i >= 0]


# âœ… Sparse search
def sparse_search(query: str, top_k: int):
    query_sparse = embed_sparse(query)
    similarities = cosine_similarity(query_sparse, corpus_sparse)[0]
    top_indices = np.argsort(similarities)[::-1][:top_k]
    return [(int(i), float(similarities[i])) for i in top_indices]


# âœ… RRF fusion
def search_rrf(query: str, top_k: int = TOP_K):
    dense_res = dense_search(query, top_k * 2)
    sparse_res = sparse_search(query, top_k * 2)

    rrf_scores = {}
    for rank, (idx, _) in enumerate(dense_res):
        rrf_scores[idx] = rrf_scores.get(idx, 0) + 1 / (rank + 1)
    for rank, (idx, _) in enumerate(sparse_res):
        rrf_scores[idx] = rrf_scores.get(idx, 0) + 1 / (rank + 1)

    sorted_rrf = sorted(rrf_scores.items(), key=itemgetter(1), reverse=True)[:top_k]

    results = []
    for idx, score in sorted_rrf:
        meta = metadata[idx]
        results.append(
            {
                "id": meta["id"],
                "text": meta["text"],
                "score": round(score, 4),
                "chunk_index": meta.get("chunk_index", -1),
                "url": meta.get("url", []),
            }
        )

    return results


# âœ… Demo
import gradio as gr


def search_interface(query):
    results = search_rrf(query)
    output = ""

    output += (
        f"ğŸ” **Query:** {results[0]['text'][:100]}...\n\n"
        if results
        else "KhÃ´ng cÃ³ káº¿t quáº£."
    )
    for idx, r in enumerate(results, 1):
        output += f"### ğŸ”¹ Káº¿t quáº£ {idx} (Score: {r['score']})\n"
        output += f"ğŸ“„ **Text**: {r['text'][:500]}...\n"
        output += f"ğŸŒ **Source**: {r['url']}\n"
        output += "---\n"
    return output


# Gradio UI
iface = gr.Interface(
    fn=search_interface,
    inputs=gr.Textbox(lines=2, placeholder="Nháº­p truy váº¥n..."),
    outputs=gr.Markdown(),
    title="ğŸ” Semantic Search (Hybrid FAISS + TF-IDF)",
    description="Nháº­p truy váº¥n tiáº¿ng Viá»‡t vÃ  xem káº¿t quáº£ tá»« FAISS + TF-IDF + RRF.",
)

if __name__ == "__main__":
    iface.launch()
